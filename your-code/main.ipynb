{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB GenAI - LLMs - OpenAI Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an assistant to answer a topic of your choosing:\n",
    " - Upload a file of your interest\n",
    " - Add Instructions to the prompt\n",
    " - Use the assistant in Playground mode\n",
    "\n",
    " https://platform.openai.com/playground/assistants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid value for parameter `type`: file. Please choose from one of: ['filepath', 'binary']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m gr\u001b[38;5;241m.\u001b[39mMarkdown(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m## File Upload Assistant\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gr\u001b[38;5;241m.\u001b[39mRow():\n\u001b[0;32m---> 35\u001b[0m     file_input \u001b[38;5;241m=\u001b[39m \u001b[43mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUpload a file of your interest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     instructions_input \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mTextbox(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdd Instructions to the prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m, lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     38\u001b[0m submit_button \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mButton(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubmit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/component_meta.py:179\u001b[0m, in \u001b[0;36mupdateable.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/components/file.py:107\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, value, file_count, file_types, type, label, every, inputs, show_label, container, scale, min_width, height, interactive, visible, elem_id, elem_classes, render, key)\u001b[0m\n\u001b[1;32m    102\u001b[0m valid_types \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    105\u001b[0m ]\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m valid_types:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid value for parameter `type`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please choose from one of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m     )\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_count \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirectory\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m file_types \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `file_types` parameter is ignored when `file_count` is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirectory\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid value for parameter `type`: file. Please choose from one of: ['filepath', 'binary']"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import openai\n",
    "import tempfile\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"YOur APi HeRE\"\n",
    "\n",
    "def process_file_and_prompt(file, instructions):\n",
    "    # Save the uploaded file to a temporary location\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "        tmp_file.write(file.read())\n",
    "        tmp_file_path = tmp_file.name\n",
    "\n",
    "    # Read the content of the uploaded file\n",
    "    with open(tmp_file_path, 'r') as f:\n",
    "        file_content = f.read()\n",
    "\n",
    "    # Combine file content with instructions\n",
    "    prompt = f\"{instructions}\\n\\nFile Content:\\n{file_content}\"\n",
    "\n",
    "    # Call OpenAI's API to get a response\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "with gr.Blocks() as interface:\n",
    "    gr.Markdown(\"## File Upload Assistant\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        file_input = gr.File(label=\"Upload a file of your interest\", type=\"file\")\n",
    "        instructions_input = gr.Textbox(label=\"Add Instructions to the prompt\", lines=4)\n",
    "    \n",
    "    submit_button = gr.Button(\"Submit\")\n",
    "    output_text = gr.Textbox(label=\"Assistant Response\", lines=10)\n",
    "\n",
    "    submit_button.click(process_file_and_prompt, inputs=[file_input, instructions_input], outputs=output_text)\n",
    "\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* Running on public URL: https://f2ad3db730ee18ed85.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f2ad3db730ee18ed85.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/blocks.py\", line 2043, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/blocks.py\", line 1590, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/utils.py\", line 865, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/tb/fw_49hkd2vl1gfskxm71_9xh0000gn/T/ipykernel_20605/2014076944.py\", line 11, in process_file_and_prompt\n",
      "    tmp_file.write(file.read())\n",
      "AttributeError: 'bytes' object has no attribute 'read'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import openai\n",
    "import tempfile\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"YOur APi HeRE\"\n",
    "\n",
    "def process_file_and_prompt(file, instructions):\n",
    "    # Save the uploaded file to a temporary location\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "        tmp_file.write(file.read())\n",
    "        tmp_file_path = tmp_file.name\n",
    "\n",
    "    # Read the content of the uploaded file\n",
    "    with open(tmp_file_path, 'r') as f:\n",
    "        file_content = f.read()\n",
    "\n",
    "    # Combine file content with instructions\n",
    "    prompt = f\"{instructions}\\n\\nFile Content:\\n{file_content}\"\n",
    "\n",
    "    # Call OpenAI's API to get a response\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "with gr.Blocks() as interface:\n",
    "    gr.Markdown(\"## File Upload Assistant\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        # Change type to \"binary\" or \"filepath\"\n",
    "        file_input = gr.File(label=\"Upload a file of your interest\", type=\"binary\")\n",
    "        instructions_input = gr.Textbox(label=\"Add Instructions to the prompt\", lines=4)\n",
    "    \n",
    "    submit_button = gr.Button(\"Submit\")\n",
    "    output_text = gr.Textbox(label=\"Assistant Response\", lines=10)\n",
    "\n",
    "    submit_button.click(process_file_and_prompt, inputs=[file_input, instructions_input], outputs=output_text)\n",
    "\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "* Running on public URL: https://b8c0809b63c5f52f10.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b8c0809b63c5f52f10.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/blocks.py\", line 2043, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/blocks.py\", line 1590, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/utils.py\", line 865, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/tb/fw_49hkd2vl1gfskxm71_9xh0000gn/T/ipykernel_20605/3006892641.py\", line 16, in process_file_and_prompt\n",
      "    file_content = f.read()\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe2 in position 10: invalid continuation byte\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/blocks.py\", line 2043, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/blocks.py\", line 1590, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/utils.py\", line 865, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/tb/fw_49hkd2vl1gfskxm71_9xh0000gn/T/ipykernel_20605/3006892641.py\", line 16, in process_file_and_prompt\n",
      "    file_content = f.read()\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe2 in position 10: invalid continuation byte\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import openai\n",
    "import tempfile\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"YOur APi HeRE\"\n",
    "\n",
    "def process_file_and_prompt(file, instructions):\n",
    "    # Save the uploaded file to a temporary location\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "        tmp_file.write(file)  # Directly write the bytes to the temporary file\n",
    "        tmp_file_path = tmp_file.name\n",
    "\n",
    "    # Read the content of the uploaded file\n",
    "    with open(tmp_file_path, 'r') as f:\n",
    "        file_content = f.read()\n",
    "\n",
    "    # Combine file content with instructions\n",
    "    prompt = f\"{instructions}\\n\\nFile Content:\\n{file_content}\"\n",
    "\n",
    "    # Call OpenAI's API to get a response\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "with gr.Blocks() as interface:\n",
    "    gr.Markdown(\"## File Upload Assistant\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        file_input = gr.File(label=\"Upload a file of your interest\", type=\"binary\")\n",
    "        instructions_input = gr.Textbox(label=\"Add Instructions to the prompt\", lines=4)\n",
    "    \n",
    "    submit_button = gr.Button(\"Submit\")\n",
    "    output_text = gr.Textbox(label=\"Assistant Response\", lines=10)\n",
    "\n",
    "    submit_button.click(process_file_and_prompt, inputs=[file_input, instructions_input], outputs=output_text)\n",
    "\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "* Running on public URL: https://34483b8b93f4b99612.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://34483b8b93f4b99612.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/blocks.py\", line 2043, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/blocks.py\", line 1590, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/utils.py\", line 865, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/tb/fw_49hkd2vl1gfskxm71_9xh0000gn/T/ipykernel_20605/4024059796.py\", line 30, in process_file_and_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Request too large for gpt-4 in organization org-W6ZOAM9PKj3sQE6QY3CQZPTH on tokens per min (TPM): Limit 10000, Requested 4718589. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/blocks.py\", line 2043, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/blocks.py\", line 1590, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/utils.py\", line 865, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/tb/fw_49hkd2vl1gfskxm71_9xh0000gn/T/ipykernel_20605/4024059796.py\", line 30, in process_file_and_prompt\n",
      "    response = openai.ChatCompletion.create(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/openai/api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Request too large for gpt-4 in organization org-W6ZOAM9PKj3sQE6QY3CQZPTH on tokens per min (TPM): Limit 10000, Requested 32337. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import openai\n",
    "import tempfile\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"YOur APi HeRE\"\n",
    "\n",
    "def process_file_and_prompt(file, instructions):\n",
    "    # Save the uploaded file to a temporary location\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "        tmp_file.write(file)  # Directly write the bytes to the temporary file\n",
    "        tmp_file_path = tmp_file.name\n",
    "\n",
    "    # Attempt to read the content of the uploaded file\n",
    "    try:\n",
    "        with open(tmp_file_path, 'r', encoding='utf-8') as f:\n",
    "            file_content = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        # If UTF-8 fails, try a different encoding or handle the binary file\n",
    "        try:\n",
    "            with open(tmp_file_path, 'r', encoding='ISO-8859-1') as f:\n",
    "                file_content = f.read()\n",
    "        except Exception as e:\n",
    "            return f\"Error reading file: {e}\"\n",
    "\n",
    "    # Combine file content with instructions\n",
    "    prompt = f\"{instructions}\\n\\nFile Content:\\n{file_content}\"\n",
    "\n",
    "    # Call OpenAI's API to get a response\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "with gr.Blocks() as interface:\n",
    "    gr.Markdown(\"## File Upload Assistant\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        file_input = gr.File(label=\"Upload a file of your interest\", type=\"binary\")\n",
    "        instructions_input = gr.Textbox(label=\"Add Instructions to the prompt\", lines=4)\n",
    "    \n",
    "    submit_button = gr.Button(\"Submit\")\n",
    "    output_text = gr.Textbox(label=\"Assistant Response\", lines=10)\n",
    "\n",
    "    submit_button.click(process_file_and_prompt, inputs=[file_input, instructions_input], outputs=output_text)\n",
    "\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "* Running on public URL: https://3be975e4295f5ecab5.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3be975e4295f5ecab5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import openai\n",
    "import tempfile\n",
    "\n",
    "openai.api_key = \"YOur APi HeRE\"\n",
    "\n",
    "def process_file_and_prompt(file, instructions):\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "        tmp_file.write(file) \n",
    "        tmp_file_path = tmp_file.name\n",
    "\n",
    "    try:\n",
    "        with open(tmp_file_path, 'r', encoding='utf-8') as f:\n",
    "            file_content = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            with open(tmp_file_path, 'r', encoding='ISO-8859-1') as f:\n",
    "                file_content = f.read()\n",
    "        except Exception as e:\n",
    "            return f\"Error reading file: {e}\"\n",
    "\n",
    "    max_input_length = 2000  \n",
    "    if len(file_content) > max_input_length:\n",
    "        file_content = file_content[:max_input_length] \n",
    "\n",
    "    prompt = f\"{instructions}\\n\\nFile Content:\\n{file_content}\"\n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response['choices'][0]['message']['content']\n",
    "    except openai.error.RateLimitError as e:\n",
    "        return f\"Rate limit exceeded: {e}\"\n",
    "\n",
    "with gr.Blocks() as interface:\n",
    "    gr.Markdown(\"## File Upload Assistant\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        file_input = gr.File(label=\"Upload a file of your interest\", type=\"binary\")\n",
    "        instructions_input = gr.Textbox(label=\"Add Instructions to the prompt\", lines=4)\n",
    "    \n",
    "    submit_button = gr.Button(\"Submit\")\n",
    "    output_text = gr.Textbox(label=\"Assistant Response\", lines=10)\n",
    "\n",
    "    submit_button.click(process_file_and_prompt, inputs=[file_input, instructions_input], outputs=output_text)\n",
    "\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk to your assistant via the API\n",
    "\n",
    "https://platform.openai.com/docs/assistants/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* Running on public URL: https://ea545112365c78cd1a.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ea545112365c78cd1a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"YOur APi HeRE\"\n",
    "\n",
    "def chat_with_assistant(user_input):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "with gr.Blocks() as interface:\n",
    "    gr.Markdown(\"## Chat with Your Assistant\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        user_input = gr.Textbox(label=\"Type your message here\", lines=4)\n",
    "        submit_button = gr.Button(\"Send\")\n",
    "    \n",
    "    output_text = gr.Textbox(label=\"Assistant Response\", lines=10)\n",
    "\n",
    "    submit_button.click(chat_with_assistant, inputs=user_input, outputs=output_text)\n",
    "\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an assistant that will call a weather API, given the user's answer and return the proper answer.\n",
    "\n",
    "See the documentation of the weather API here: https://open-meteo.com/en/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'latitude': 52.52, 'longitude': 13.419998, 'generationtime_ms': 0.06699562072753906, 'utc_offset_seconds': 0, 'timezone': 'GMT', 'timezone_abbreviation': 'GMT', 'elevation': 38.0, 'hourly_units': {'time': 'iso8601', 'temperature_2m': '°C'}, 'hourly': {'time': ['2024-06-17T00:00', '2024-06-17T01:00', '2024-06-17T02:00', '2024-06-17T03:00', '2024-06-17T04:00', '2024-06-17T05:00', '2024-06-17T06:00', '2024-06-17T07:00', '2024-06-17T08:00', '2024-06-17T09:00', '2024-06-17T10:00', '2024-06-17T11:00', '2024-06-17T12:00', '2024-06-17T13:00', '2024-06-17T14:00', '2024-06-17T15:00', '2024-06-17T16:00', '2024-06-17T17:00', '2024-06-17T18:00', '2024-06-17T19:00', '2024-06-17T20:00', '2024-06-17T21:00', '2024-06-17T22:00', '2024-06-17T23:00', '2024-06-18T00:00', '2024-06-18T01:00', '2024-06-18T02:00', '2024-06-18T03:00', '2024-06-18T04:00', '2024-06-18T05:00', '2024-06-18T06:00', '2024-06-18T07:00', '2024-06-18T08:00', '2024-06-18T09:00', '2024-06-18T10:00', '2024-06-18T11:00', '2024-06-18T12:00', '2024-06-18T13:00', '2024-06-18T14:00', '2024-06-18T15:00', '2024-06-18T16:00', '2024-06-18T17:00', '2024-06-18T18:00', '2024-06-18T19:00', '2024-06-18T20:00', '2024-06-18T21:00', '2024-06-18T22:00', '2024-06-18T23:00', '2024-06-19T00:00', '2024-06-19T01:00', '2024-06-19T02:00', '2024-06-19T03:00', '2024-06-19T04:00', '2024-06-19T05:00', '2024-06-19T06:00', '2024-06-19T07:00', '2024-06-19T08:00', '2024-06-19T09:00', '2024-06-19T10:00', '2024-06-19T11:00', '2024-06-19T12:00', '2024-06-19T13:00', '2024-06-19T14:00', '2024-06-19T15:00', '2024-06-19T16:00', '2024-06-19T17:00', '2024-06-19T18:00', '2024-06-19T19:00', '2024-06-19T20:00', '2024-06-19T21:00', '2024-06-19T22:00', '2024-06-19T23:00', '2024-06-20T00:00', '2024-06-20T01:00', '2024-06-20T02:00', '2024-06-20T03:00', '2024-06-20T04:00', '2024-06-20T05:00', '2024-06-20T06:00', '2024-06-20T07:00', '2024-06-20T08:00', '2024-06-20T09:00', '2024-06-20T10:00', '2024-06-20T11:00', '2024-06-20T12:00', '2024-06-20T13:00', '2024-06-20T14:00', '2024-06-20T15:00', '2024-06-20T16:00', '2024-06-20T17:00', '2024-06-20T18:00', '2024-06-20T19:00', '2024-06-20T20:00', '2024-06-20T21:00', '2024-06-20T22:00', '2024-06-20T23:00', '2024-06-21T00:00', '2024-06-21T01:00', '2024-06-21T02:00', '2024-06-21T03:00', '2024-06-21T04:00', '2024-06-21T05:00', '2024-06-21T06:00', '2024-06-21T07:00', '2024-06-21T08:00', '2024-06-21T09:00', '2024-06-21T10:00', '2024-06-21T11:00', '2024-06-21T12:00', '2024-06-21T13:00', '2024-06-21T14:00', '2024-06-21T15:00', '2024-06-21T16:00', '2024-06-21T17:00', '2024-06-21T18:00', '2024-06-21T19:00', '2024-06-21T20:00', '2024-06-21T21:00', '2024-06-21T22:00', '2024-06-21T23:00', '2024-06-22T00:00', '2024-06-22T01:00', '2024-06-22T02:00', '2024-06-22T03:00', '2024-06-22T04:00', '2024-06-22T05:00', '2024-06-22T06:00', '2024-06-22T07:00', '2024-06-22T08:00', '2024-06-22T09:00', '2024-06-22T10:00', '2024-06-22T11:00', '2024-06-22T12:00', '2024-06-22T13:00', '2024-06-22T14:00', '2024-06-22T15:00', '2024-06-22T16:00', '2024-06-22T17:00', '2024-06-22T18:00', '2024-06-22T19:00', '2024-06-22T20:00', '2024-06-22T21:00', '2024-06-22T22:00', '2024-06-22T23:00', '2024-06-23T00:00', '2024-06-23T01:00', '2024-06-23T02:00', '2024-06-23T03:00', '2024-06-23T04:00', '2024-06-23T05:00', '2024-06-23T06:00', '2024-06-23T07:00', '2024-06-23T08:00', '2024-06-23T09:00', '2024-06-23T10:00', '2024-06-23T11:00', '2024-06-23T12:00', '2024-06-23T13:00', '2024-06-23T14:00', '2024-06-23T15:00', '2024-06-23T16:00', '2024-06-23T17:00', '2024-06-23T18:00', '2024-06-23T19:00', '2024-06-23T20:00', '2024-06-23T21:00', '2024-06-23T22:00', '2024-06-23T23:00'], 'temperature_2m': [16.7, 16.1, 15.4, 15.3, 15.5, 16.6, 18.5, 19.1, 20.1, 21.1, 21.1, 22.3, 23.4, 23.5, 24.1, 22.4, 21.7, 22.3, 21.8, 21.0, 19.3, 18.3, 17.8, 16.9, 16.4, 16.1, 15.9, 15.9, 16.2, 17.1, 18.6, 20.1, 21.3, 22.7, 23.9, 24.6, 25.1, 25.4, 25.8, 26.2, 26.0, 25.5, 23.4, 21.2, 19.1, 18.4, 18.1, 17.7, 17.4, 17.1, 16.8, 17.2, 16.5, 16.7, 16.1, 16.1, 15.4, 15.7, 16.0, 16.3, 16.9, 16.7, 17.9, 17.4, 19.3, 19.2, 18.5, 18.6, 17.6, 16.3, 15.1, 14.0, 13.1, 12.2, 11.4, 10.6, 10.3, 10.9, 12.2, 14.0, 15.9, 17.7, 19.2, 20.5, 21.6, 22.4, 23.0, 23.2, 23.2, 23.0, 22.5, 21.3, 19.9, 18.7, 18.1, 17.8, 17.5, 16.9, 16.3, 16.1, 16.5, 17.3, 18.5, 20.3, 22.4, 24.4, 25.9, 27.3, 28.3, 29.1, 29.5, 29.2, 27.5, 25.2, 23.2, 22.2, 21.7, 21.3, 21.1, 21.1, 20.9, 20.3, 19.5, 18.8, 18.2, 17.7, 17.6, 18.4, 19.7, 20.6, 20.7, 20.5, 20.4, 18.2, 17.0, 16.2, 16.1, 16.3, 16.3, 16.0, 15.6, 15.1, 14.5, 13.9, 13.5, 13.2, 13.1, 13.2, 13.5, 14.0, 14.8, 16.0, 17.4, 18.6, 19.3, 19.8, 20.1, 20.4, 20.5, 20.6, 20.7, 20.8, 20.6, 20.1, 19.3, 18.4, 17.4, 16.4]}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_weather_forecast(latitude, longitude):\n",
    "    base_url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"hourly\": \"temperature_2m\"\n",
    "    }\n",
    "    response = requests.get(base_url, params=params)\n",
    "    return response.json()\n",
    "\n",
    "# Example usage:\n",
    "forecast = get_weather_forecast(52.52, 13.41)\n",
    "print(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7870\n",
      "* Running on public URL: https://d4eaad23388f95259f.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d4eaad23388f95259f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "\n",
    "def get_weather_forecast(latitude, longitude):\n",
    "    base_url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"current_weather\": True  # Get current weather\n",
    "    }\n",
    "    response = requests.get(base_url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        weather_data = response.json()\n",
    "        current_weather = weather_data['current_weather']\n",
    "        return f\"Temperature: {current_weather['temperature']}°C\\n\" \\\n",
    "               f\"Wind Speed: {current_weather['windspeed']} km/h\\n\" \\\n",
    "               f\"Wind Direction: {current_weather['winddirection']}°\\n\" \\\n",
    "               f\"Weather Code: {current_weather['weathercode']}\"\n",
    "    else:\n",
    "        return \"Error fetching weather data. Please check the coordinates.\"\n",
    "\n",
    "with gr.Blocks() as interface:\n",
    "    gr.Markdown(\"## Weather Assistant\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        latitude_input = gr.Textbox(label=\"Enter Latitude\", placeholder=\"e.g., 52.52\")\n",
    "        longitude_input = gr.Textbox(label=\"Enter Longitude\", placeholder=\"e.g., 13.41\")\n",
    "    \n",
    "    submit_button = gr.Button(\"Get Weather\")\n",
    "    output_text = gr.Textbox(label=\"Weather Forecast\", lines=10)\n",
    "\n",
    "    submit_button.click(get_weather_forecast, inputs=[latitude_input, longitude_input], outputs=output_text)\n",
    "\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7871\n",
      "* Running on public URL: https://afadf536cde64bca8d.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://afadf536cde64bca8d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/blocks.py\", line 2043, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/blocks.py\", line 1590, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/utils.py\", line 865, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/tb/fw_49hkd2vl1gfskxm71_9xh0000gn/T/ipykernel_20605/3689945213.py\", line 51, in chat_with_assistant\n",
      "    weather_info = get_weather_forecast(location)\n",
      "  File \"/var/folders/tb/fw_49hkd2vl1gfskxm71_9xh0000gn/T/ipykernel_20605/3689945213.py\", line 13, in get_weather_forecast\n",
      "    if geocode_data['results']:\n",
      "KeyError: 'results'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/blocks.py\", line 2043, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/blocks.py\", line 1590, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/utils.py\", line 865, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/tb/fw_49hkd2vl1gfskxm71_9xh0000gn/T/ipykernel_20605/3689945213.py\", line 51, in chat_with_assistant\n",
      "    weather_info = get_weather_forecast(location)\n",
      "  File \"/var/folders/tb/fw_49hkd2vl1gfskxm71_9xh0000gn/T/ipykernel_20605/3689945213.py\", line 13, in get_weather_forecast\n",
      "    if geocode_data['results']:\n",
      "KeyError: 'results'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/blocks.py\", line 2043, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/blocks.py\", line 1590, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Users/majed-abdulkareem/opt/anaconda3/envs/FTF2/lib/python3.10/site-packages/gradio/utils.py\", line 865, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/tb/fw_49hkd2vl1gfskxm71_9xh0000gn/T/ipykernel_20605/3689945213.py\", line 51, in chat_with_assistant\n",
      "    weather_info = get_weather_forecast(location)\n",
      "  File \"/var/folders/tb/fw_49hkd2vl1gfskxm71_9xh0000gn/T/ipykernel_20605/3689945213.py\", line 13, in get_weather_forecast\n",
      "    if geocode_data['results']:\n",
      "KeyError: 'results'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"YOur APi HeRE\"\n",
    "\n",
    "def get_weather_forecast(location):\n",
    "    geocode_url = f\"https://geocoding-api.open-meteo.com/v1/search?name={location}\"\n",
    "    geocode_response = requests.get(geocode_url)\n",
    "    \n",
    "    if geocode_response.status_code == 200:\n",
    "        geocode_data = geocode_response.json()\n",
    "        if geocode_data['results']:\n",
    "            latitude = geocode_data['results'][0]['latitude']\n",
    "            longitude = geocode_data['results'][0]['longitude']\n",
    "            \n",
    "            weather_url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "            params = {\n",
    "                \"latitude\": latitude,\n",
    "                \"longitude\": longitude,\n",
    "                \"current_weather\": True\n",
    "            }\n",
    "            weather_response = requests.get(weather_url, params=params)\n",
    "            \n",
    "            if weather_response.status_code == 200:\n",
    "                weather_data = weather_response.json()\n",
    "                current_weather = weather_data['current_weather']\n",
    "                return f\"Current weather in {location}:\\n\" \\\n",
    "                       f\"Temperature: {current_weather['temperature']}°C\\n\" \\\n",
    "                       f\"Wind Speed: {current_weather['windspeed']} km/h\\n\" \\\n",
    "                       f\"Wind Direction: {current_weather['winddirection']}°\\n\" \\\n",
    "                       f\"Weather Code: {current_weather['weathercode']}\"\n",
    "            else:\n",
    "                return \"Error fetching weather data.\"\n",
    "        else:\n",
    "            return \"Location not found.\"\n",
    "    else:\n",
    "        return \"Error fetching location data.\"\n",
    "\n",
    "def chat_with_assistant(user_input):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    )\n",
    "    assistant_response = response['choices'][0]['message']['content']\n",
    "    \n",
    "    if \"weather\" in user_input.lower():\n",
    "        location = user_input.split(\"weather in\")[-1].strip()  \n",
    "        weather_info = get_weather_forecast(location)\n",
    "        return f\"{assistant_response}\\n\\n{weather_info}\"\n",
    "    \n",
    "    return assistant_response\n",
    "\n",
    "with gr.Blocks() as interface:\n",
    "    gr.Markdown(\"## Chat with Your Weather Assistant\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        user_input = gr.Textbox(label=\"Type your message here\", lines=4)\n",
    "        submit_button = gr.Button(\"Send\")\n",
    "    \n",
    "    output_text = gr.Textbox(label=\"Assistant Response\", lines=10)\n",
    "\n",
    "    submit_button.click(chat_with_assistant, inputs=user_input, outputs=output_text)\n",
    "\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want to, there is a hint here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Chatbots / Assistants have a way to respond in json format. \n",
    "\n",
    "Explore the function calling functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ironhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
